# -*- coding: utf-8 -*-
"""sci kit learn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XMH9e90pXGd2Pi-GrYr1ASLP5HzBtgzg
"""

import sklearn

print('the scikit-learn version is ',sklearn.__version__)

"""fit and predict"""

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(random_state =42)

X = [[ 1,  2,  3],
    [11, 12, 13]]

y = [0, 1]


model.fit(X,y)

model.predict(X)

model.predict([[14, 15, 16],[4, 5, 6] ])

model.predict_proba([[14, 15, 16],[4, 5, 6] ])

"""one more example

"""

from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression #Fixed typo here

X,y = load_iris (return_X_y = True)

clf = LogisticRegression(random_state =0 , max_iter = 1000).fit(X,y)

clf.predict(X[:2, :])

clf.predict_proba(X[:1, :])

X[:1, :]

y[:1]

clf.score(X,y)

"""Transformers and pre-processors"""

from sklearn.preprocessing import StandardScaler
import numpy as np

X=np.array([1,2,3,4,5,6,7]).reshape(-1,1)

Fx=StandardScaler().fit(X)

Fx.transform(X)

Y = np.array([1.1,2.2,3.3,4.4,5.5,6.6]).reshape(-1, 1)

Fx.transform(Y)

x.shape

import pandas as pd
X = pd.DataFrame(
    {'city': ['London', 'London', 'Paris', 'Sallisaw'],
     'title': ["His Last Bow", "How Watson Learned the Trick",
     "A Moveable Feast", "The Grapes of Wrath"],
     'expert_rating': [5, 3, 4, 5],
     'user_rating': [4, 5, 4, 3]})

X

from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import OneHotEncoder
column_trans = ColumnTransformer(
    [('categories', OneHotEncoder(dtype='int'), ['city']),
    ('title_bow', CountVectorizer(), 'title')],
    remainder='drop', verbose_feature_names_out=False)

column_trans.fit(X)

"""PIPELINE"""

from sklearn.preprocessing import StandardScaler , MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

#create a pipeline object
pipe = make_pipeline(
    StandardScaler(),
    MinMaxScaler(),
    LogisticRegression()
    )

# load the iris dataset and split it into train and test sets
X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

# fit the whole pipeline
pipe.fit(X_train, y_train)

accuracy_score(pipe.predict(X_test), y_test)

"""Model Evaluation

"""

from sklearn.datasets import make_regression
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_validate

X, y = make_regression(n_samples=100, random_state=0)
lr = LinearRegression()

result = cross_validate(lr, X, y)  # defaults to 5-fold CV

X

y

result['test_score']  # r_squared score

"""Automated Parameter search

"""

from sklearn.datasets import fetch_california_housing
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import train_test_split
from scipy.stats import randint

X, y = fetch_california_housing(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

# define the parameter space that will be searched over
param_distributions = {'n_estimators': randint(1, 5),
                       'max_depth': randint(5, 10)}

# now create a searchCV object and fit it to the data
search = RandomizedSearchCV(estimator=RandomForestRegressor(random_state=0),
                             n_iter=5,
                             param_distributions=param_distributions,
                             random_state=0)

search.fit(X_train, y_train)

search.best_params_

# the search object now acts like a normal random forest estimator
# with max_depth=9 and n_estimators=4
search.score(X_test, y_test)

search.best_estimator_.score(X_test, y_test)